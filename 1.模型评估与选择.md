### 1.经验差与过拟合
&nbsp; &nbsp; &nbsp; &nbsp; 学习器的实际预测输出与样本的真实输出之间的差异称为“误差”，学习器在训练集上的误差成为“训练误差”或“经验误差”，在新样本上的误差称为“泛化误差”。  </br>
&nbsp; &nbsp; &nbsp; &nbsp;学习器将训练样本自身的一些特点当作了所有样本都具有的一般性质，这样导致泛化性能下降，称之为“过拟合”。与“过拟合”相对的是“欠拟合”，是指对训练样本的一般性质尚未学好。
### 2.评估方法
&nbsp; &nbsp; &nbsp; &nbsp; 对学习器的泛化误差进行评估时，需要用一个“测试集”来测试学习器对样本的判别能力，测试集尽可能与训练集互斥。 </br> 
&nbsp; &nbsp; &nbsp; &nbsp; 假设现在有一个包含m个样本的数据集 D = {（x1, y1）, (x2, y2),...,(xm, ym)}，对D进行适当处理，从中产出训练集S和测试集T。
#### （1）留出法
&nbsp; &nbsp; &nbsp; &nbsp; 直接将数据集D划分为两个互斥的集合，其中一个作为训练集S，另一个作为测试集T。需要注意的是，训练/测试集的划分要尽可能保持数据分布的一致性，避免因为数据划分过程引入额外偏差而对最终结果产生影响。在分类任务中，采用“分层采样”保持样本类别比例相似。另一个需要注意的问题是，单次使用留出法得到的估计结果往往不够稳定可靠，一般采用若干次随机划分，重复实验取平均值作为留出法的评估结果。  </br>
&nbsp; &nbsp; &nbsp; &nbsp; 留出法的一个窘境是：若训练集 >> 测试集，则模型更接近于用D训练出来的模型，由于T比较小，评估结果可能不够稳定准确；若T多包含一些样本，则S与T差别更大了，被评估模型与用D训练出来的模型可能有较大差别，从而降低了评估结果的保真性（fidelity）。这个问题没有完美的解决方案，常见做法是将2/3 ~ 4/5的样本用于训练，剩余样本用于测试。
#### （2）交叉验证
&nbsp; &nbsp; &nbsp; &nbsp; 先将D划分为k个大小相似的互斥子集，每次用k-1个子集的并集当作训练集，余下的子集作为测试集，这样可以获得k组训练/测试集，进行k次训练和测试，最终返回这k个测试结果的均值，这一方法称为交叉验证法，又叫做“k折交叉验证”。k常用的取值是5、10、20，与留出法相似，交叉验证也要重复p次取均值，常见的有“10次10折交叉验证”。 </br> 
例子D = {D1, D2, D3, D4, D5} 5折交叉验证：  </br>

| 训练集 | 测试集 | 结果 |
| :------: | :------: | :------: |
| {D1, D2, D3, D4} | {D5} | result1 |
| {D1, D2, D3, D5} | {D4} | result2 |
| {D1, D2, D4, D5} | {D3} | result3 |
| {D1, D3, D4, D5} | {D2} | result4 |
| {D2, D3, D4, D5} | {D1} | result5 | 

resultAvg = (result1 + result2 + result3 + result4 + result5) / 5  
#### （3）自助法
&nbsp; &nbsp; &nbsp; &nbsp; 每次从D中随机挑选一个样本，拷贝放入D′，再放回D，重复m次，就得到了包含m个样本的D′，这就是自助采样的结果。样本在m次采样中始终不被采到的概率为（1 - 1/m）^m，取极限得到这个值等于1/e，约有36.8%的样本未出现在D′中，于是可以将D′用作训练集，D\D′用作测试集；这样实际评估的模型与期望评估的模型都使用m个训练样本，但仍有约1/3的没有出现在训练集中的样本用于测试，这样的测试结果称为“包外估计”。自助法在数据集较小，难以有效划分训练集/测试集时很有用，在数据量足够时，留出法和交叉验证法更常用。</br>
### 3.性能度量（performance measure）
&emsp; &emsp; 回归任务最常用的性能度量是“均方误差”（mean squared error）:</br>
![image](https://github.com/Xenophus/MachineLearning/blob/master/images/MSE.png)
#### （1）错误率与精度
&emsp; &emsp; 错误率定义：</br>
&emsp; ![image](https://github.com/Xenophus/MachineLearning/blob/master/images/Error.png)</br>
&emsp; &emsp; 精度定义：</br>
![image](https://github.com/Xenophus/MachineLearning/blob/master/images/Acc.png)</br>
#### （2）查准率、查全率与F1
&emsp; &emsp; 给出分类结果混淆矩阵：
<table>
  <tr>
    <td rowspan=2>真实情况</td>
    <td colspan=2>预测结果</td>
  </tr>
  <tr>
    <td>正例</td>
    <td>反例</td>
  </tr>
  <tr>
    <td>正例</td>
    <td>TP（真正例）</td>
    <td>FN（假反例）</td>
  </tr>
  <tr>
    <td>反例</td>
    <td>FP（假正例）</td>
    <td>TN（真反例）</td>
  </tr>
</table>
&emsp; &emsp; 查准率P和查全率R分别定义为：</br>

![image](https://github.com/Xenophus/MachineLearning/blob/master/images/PR.png)  
&emsp; &emsp; 查准率是指预测的正例有多少是正确的，查全率是指实际情况的正例有多少被预测到了。查准率和查全率是一对相互矛盾的度量。一般来说查准率高时，查全率往往偏低；而查全率高时，查准率往往偏低。只有在一些简单的任务中，才会使查准率和查全率都很高。  
&emsp; &emsp; P-R曲线与平衡点示意图：

![image](https://github.com/Xenophus/MachineLearning/blob/master/images/PRCurve.png)  

&emsp; &emsp; 若一个学习器的P-R曲线被另一个学习器完全保住，则后者的性能优于前者，图中A优于C。对于A、B这种发生交叉的情况，一般很难说谁好谁坏。为了综合考虑查准率和查全率的性能度量，采用平衡点（Break-Event Point，BEP）来进行衡量，平衡点是查准率等于查全率时的取值，这时可以判断出A学习器优于B学习器。然而BEP还是过于简化了些。  
&emsp; &emsp; 综合P、R衡量一个学习器最常用的是F1度量，是基于查准率和查全率的调和平均定义的 1/F1 = 1/2 * (1/P + 1/R)，即：  

![image](https://github.com/Xenophus/MachineLearning/blob/master/images/F1.png)  
&emsp; &emsp; 在一些应用中对查准率和查全率的重视程度有所不同。例如在商品推荐系统中，为了尽可能少地打扰用户，更希望推荐内容是客户感兴趣的，此时查准率更重要；而在逃犯信息检索系统中，更希望尽可能少漏掉逃犯，此时查全率更重要。F1度量的一般形式——Fβ，能表达出对查准率和查全率的不同偏好，定义为：  

![image](https://github.com/Xenophus/MachineLearning/blob/master/images/Fβ.png)  
&emsp; &emsp; β < 1 时，查准率有更大影响；β = 1 时，退化为标准的F1；β > 1 时，查全率有更大影响。  
&emsp; &emsp; 在n个二分类混淆矩阵上综合考察查准率和查全率，有两种做法。  
&emsp; &emsp; 一种是直接在各混淆矩阵上计算出查准率和查全率，在计算平均值，得到“宏查准率”、“宏查全率”和“宏F1”：  

![image](https://github.com/Xenophus/MachineLearning/blob/master/images/macro.png)  
&emsp; &emsp; 另一种是将各混淆矩阵的对应元素平均，得到TP、FN、FP、TN的均值，然后得到“微查准率”、“微查全率”和“微F1”：

![image](https://github.com/Xenophus/MachineLearning/blob/master/images/micro.png)  
#### （3）ROC 与 AUC
&emsp; &emsp; 真正利率（True Positive Rate，简称TPR），真正例与真实正例的比例：TPR = TP / TP + FN，假正例率（False Positive Rate，简称FPR），假正例与真实反例的比例：FPR = FP / FP + TN。  
&emsp; &emsp; 以TPR为纵轴，FPR为横轴，可做ROC（Receiver Operating Characteristic，受试者工作特征）曲线。  

![image](https://github.com/Xenophus/MachineLearning/blob/master/images/ROC.png)   
&emsp; &emsp; AUC（Area Under ROC Curve），是ROC曲线下的面积，通过比较AUC可比较学习器的优劣。假定ROC的曲线是由坐标{(x1, y1), (x2, y2), ..., (xm, ym)}的点按顺序连接而形成(x1=0, xm=1)，则AUC可估算为：

![image](https://github.com/Xenophus/MachineLearning/blob/master/images/AUC.png)   
